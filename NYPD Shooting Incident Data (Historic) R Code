---
title: "NYPD Shooting Incident Data (Historic)"
author: "Mohamed Essam"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## Steps:
### 1. Data Source Information
### 2. Environment Setup
### 3. Transformation and Exploratory Data Analysis (EDA)
### 4. Feature Removal and Renaming
### 5. Check for Duplicates and Remove
### 6. Change Feature Class Types
### 7. More Feature Checks
### 8. EDA Cont.
### 9. Visualizations
### 10. Model
### 11. Conclusion 
### 12. Bias

## Setup

Note, before using knitr please install all missing packages from the code cell below into your environment. 

R Packages used:

- library(tidyverse)
- library(lubridate)
- library(ggplot2)
- library(ggmap)
- library(gridExtra)

### Data Source Information

This data was procured from <https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic> and will be updated annually at the source and any time this .rmd file is knit.

From the source:
*"List of every shooting incident that occurred in NYC going back to 2006 through the end of the previous calendar year.*

*This is a breakdown of every shooting incident that occurred in NYC going back to 2006 through the end of the previous calendar year. This data is manually extracted every quarter and reviewed by the Office of Management Analysis and Planning before being posted on the NYPD website. Each record represents a shooting incident in NYC and includes information about the event, the location and time of occurrence. In addition, information related to suspect and victim demographics is also included. This data can be used by the public to explore the nature of shooting/criminal activity. Please refer to the attached data footnotes for additional information about this dataset."*



### Environment Setup

We will first begin by loading in the R packages we intend to use. 

Then, we will import the data using a URL directly from the source, this ensures we will capture updates to the data as they come in, whenever this is run again.

```{r Setup RMD}
# Output all commands run and set a standard plot size
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 6)

library(tidyverse)
library(lubridate)
library(ggmap)
library(ggplot2)
library(gridExtra)
library(tinytex)

import_url <- read.csv("https://data.cityofnewyork.us/api/views/833y-fsy8/rows.csv?accessType=DOWNLOAD")
```

## Transformation and Exploratory Data Analysis (EDA)

Let's take a look at the dimensions of this imported data.frame, as well as the variable types of each column, and output a summary.
```{r EDA}
dim(import_url)
str(import_url)
summary(import_url)
```


### Feature Removal and Renaming
  
First, let's remove any features that we won't be needing for our analysis.

1. `JURISDICTION_CODE` is pretty broad for localizing shooting incidents so we will end up using `BORO` instead which will give more insight to our analysis.
2. `X_COORD_CD`, `Y_COORD_CD`, and `Lon_Lat` are all redundant as we will use `LONGITUDE` and `LATITUDE` in their place.

Also, let's rename a few of these for more readability.
```{r Feature Removal and Renaming}
# Remove features
import_url <- select(import_url, -JURISDICTION_CODE, -X_COORD_CD, -Y_COORD_CD, -Lon_Lat)
# Rename features
import_url <- import_url %>%
        rename(c('DATE' = 'OCCUR_DATE', 'TIME' = 'OCCUR_TIME','BOROUGH' = 'BORO', 
                'LOCATION' = 'LOCATION_DESC', 'MURDER_FLAG' = 'STATISTICAL_MURDER_FLAG', 
                'PERP_AGE' = 'PERP_AGE_GROUP', 'VICTIM_AGE' = 'VIC_AGE_GROUP', 'VICTIM_SEX' = 'VIC_SEX',
                'VICTIM_RACE' = 'VIC_RACE', 'LATITUDE' = 'Latitude', 'LONGITUDE' = 'Longitude'))
head(import_url)
```

### Check for Duplicates and Remove
Next, we will check if there are any missing or duplicated data points, focusing only on the `INCIDENT_KEY` feature for now.
This feature will be the most important for identifying any duplicate entries as they should all be unique.
```{r NA or Null and Duplicates}
# Check for any NA or Null values
any(is.na(import_url$INCIDENT_KEY)) | any(is.null(import_url$INCIDENT_KEY))
# Check for duplicates
length(unique(import_url$INCIDENT_KEY))
length(import_url$INCIDENT_KEY)
```
Subtracting the results here shows that there are **5470 duplicate data points**! 
Let's take a closer look to make sure these aren't false positives.
```{r Removal Duplicates}
# Query duplicates to see what they look like
head(filter(import_url, duplicated(import_url$INCIDENT_KEY)))
# Check a few entries
arrange(filter(import_url, INCIDENT_KEY == 227647476 | INCIDENT_KEY == 232390408), INCIDENT_KEY)
# Yes those are definitely duplicates
# Remove duplicates
import_url <- filter(import_url, !duplicated(import_url$INCIDENT_KEY))
# Check work
length(duplicated(import_url$INCIDENT_KEY))
```
That should do it for the duplicated data points. Let's continue our transformations.

### Change Feature Class Types
For better analysis we should change the class type of a few of these features to make them easier to work with.
```{r Class Changes}
# Character to Date and Period
import_url <- import_url %>%
        mutate(DATE = mdy(DATE)) %>%
        mutate(TIME = hms(TIME))

# Character to Factors - changes all character columns to factor
import_url <- import_url %>%
        mutate(across(where(is.character), as.factor))
str(import_url)
```

### More Feature Checks
We will continue to look at the features and see if any of these blank entries will cause trouble during the analysis.
Also, we'll look to see if there are any duplicate categorical factors in the rest of the features.
```{r More transformations}
# Create a table of each column to check factor levels
for (i in 1:length(import_url)){
    ifelse(is.factor(import_url[ ,i]), print(table(import_url[ ,i, drop = FALSE])), next)
}
```
Looks like there are quite a few blank entries and a few labeled as "UNKNOWN". 

- Over 50% of the `LOCATION` and `PERP_AGE` data is unknown.
- `BOROUGH`, `MURDER_FLAG`, `PRECINCT`, `DATE`, `TIME`, `LONGITUDE`, and `LATITUDE` have no missing entries.
- The remaining features are missing a few, but are not a significant amount compared to the overall size of the dataset.

We will combine these by labeling all blanks as "UNKNOWN". This data likely comes from officers on the scene who may have: 
1.missed some information; 
2. did not have a witness and/or did not catch the offender; or
3. not have previously recorded this particular data but now are due to process changes. 
It's reasonable to leave these missing data points in because the missing data also gives us information about that incident.
Also, removing these data points due to their empty entries would be a mistake considering the data that is complete 
holds more relevance. Removing it would be like throwing away the baby with the bathwater 
(e.g. removing a data point that is missing the `LOCATION` description, but isn't missing the rest of the information will only hurt our analysis. Especially considering `BOROUGH`,`LONGITUDE`, and`LATITUDE` aren't missing).

Here we will combine and correct any missing entries with the methods discussed above.
```{r Dealing with the Unknown}
for (i in 1:length(import_url)){
    # IF the column is a factor AND contains empty space OR a 'U'
    if(is.factor(import_url[ ,i]) && '' %in% import_url[ ,i] || 'U' %in% import_url[ ,i]){
        # Add level named UNKNOWN
        levels(import_url[, i]) <- c(levels(import_url[, i]), 'UNKNOWN')
        # Replace missing values and Us with UNKNWON
        import_url[, i][import_url[, i] == ''] <- as.factor('UNKNOWN')
        import_url[, i][import_url[, i] == 'U'] <- as.factor('UNKNOWN')
        # Remove unused levels
        import_url[, i] <- droplevels(import_url[, i])
    }else{
        next
    }
}
```
Lastly there are a few values in `PERP_AGE` that look like data entry typos. 
See table above:('1020', '224', '940') 
Here, we cannot assume what was intended so we will change these age values to 'UNKNOWN'.
```{r PERP_AGE Anomalies}
# Before changes
table(import_url$PERP_AGE)
# Set values we want to keep as levels
age_levels <- c('<18', '18-24', '25-44', '45-64', '65+', 'UNKNOWN')
# Find all values NOT in age_levels (notice the !)
import_url$PERP_AGE[!import_url$PERP_AGE %in% age_levels] <- as.factor('UNKNOWN')
# Remove unused levels
import_url$PERP_AGE <- droplevels(import_url$PERP_AGE)
# After changes
table(import_url$PERP_AGE)
```

### EDA Cont.

Let's take a look at a summary of this data now that we've cleaned it up.
```{r Summary}
summary(import_url)
```
Changing many of these features from characters into factors really improves R's ability to summarize the data here.
We can make many conclusions about this data set just from looking at the summary.

- The earliest data point is from January 1, 2006.
- The most recent is from December29, 2023.
- The distribution of `DATE` is slightly right-skewed, with a mean 9 months in the future of the median. This suggests there were more incidents from 2006-2014 than 2014-2021.
- Out of the 22,394 shooting incidents, 3903 are associated with a murder, ~17.4%.
- Majority of perpetrators are between the ages of 18-24 and 25-44.
- The vast majority of identified perpetrators are male.
- The victim's age are a little more equally distributed than the perp's but the vast majority are also between 18-44.

Let's take a closer look and check the `DATE` distribution skewness by finding the midpoint date of the data set (this is the exact middle date of this data set).
```{r Date Midpoint}
data_interval <- interval(min(import_url$DATE), max(import_url$DATE))
int_start(data_interval) + (int_end(data_interval) - int_start(data_interval)) / 2

median(import_url$DATE)
```
Interesting, this proves there were more incidents in the first half of the time interval of the data set. 
We will look into this further during visualizations.

## Visualizations

Now that the transformations are complete, let's start plotting these features against each other and visualizing our data 
so we can try to make some conclusions and more clearly inform them.

Let's begin by visualizing the number of shooting incidents by the `BOROUGH` they occurred in.
```{r, Visualizations - Incidents per Borough, fig.height = 4, fig.width = 8, fig.align = 'center'}
# Order factor levels of BOROUGH based on frequency
import_url$BOROUGH <- fct_infreq(import_url$BOROUGH)
# Plot graph
import_url %>%
   ggplot(., aes(x = reorder(BOROUGH, BOROUGH, length, decreasing = TRUE), fill = BOROUGH)) +
   geom_bar(aes(y = after_stat(count))) +
   scale_fill_brewer(palette = 'watlington', direction = -1) +
   theme(axis.text.x = element_text(angle = 25, hjust = 1)) +
   labs(title = 'Number of Shooting Incidents by Borough',
        x = 'Borough', y = 'Number of Incidents',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
```

As you can see the majority of shooting incidents have occurred in Brooklyn followed by the Bronx and Queens.

Now to plot the number of shooting incidents organized by the `LOCATION` description assigned to them.
```{r Visualizations - Incidents per Location}
import_url %>%
   ggplot(., aes(x = reorder(LOCATION, LOCATION, length, decreasing = TRUE), fill = LOCATION)) + 
   geom_bar(aes(y = after_stat(count)), show.legend = FALSE) +
   theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
   scale_y_log10() +
   labs(title = 'Number of Shooting Incidents by Location Description',
        x = 'Location Description', y = 'Number of Incidents (Log Scaled)',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
```
Something to note, this graph has been logarithmically scaled on the y-axis for easier viewing.

- "Multi dwell-public housing" shows the most shooting incidents, followed by "Multi-dwell-apt building", and "Pvt house".
- This graph shows evidence that a large portion of shooting incidents in NYC occur near the homes of those involved, and though we can't say for sure, this suggests altercations are not between complete strangers. 
- This also shows just how many of these incidents were left without a location description. This could have been left empty for many reasons -- a shooting was reported but it was unclear exactly from where, it occurred on the street, etc. 
Again, it is still important to look at the remaining data we do have as it is still quite a sizeable sample.


```{r Total Incidents per Month and Hour, fig.align="center", echo = FALSE,fig.width = 12}

# Group by month and total # of incidents
inc_month_totals <- import_url %>%
    group_by(month_totals = months(DATE)) %>%
    summarize(n = n()) %>%
    arrange(match(month_totals, month.name)) %>%
    mutate(month_totals = factor(month_totals, levels = month.name))

# Plot the tibble
month_plot <- ggplot(inc_month_totals, aes(x = month_totals, y = n, fill = month_totals)) +
    geom_col()+
    scale_fill_brewer(palette = 'Paired', direction = 1, name = 'Month') +
    theme(axis.text.x = element_text(angle = 45, hjust = 0.5)) +
    labs(title = 'Shooting Incidents per Month',
        x = 'Month', y = 'Number of Incidents',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')

# Group by hour and total # of incidents
inc_hour_totals <- import_url %>%
    group_by(hour_totals = hour(TIME)) %>%
    summarize(n = n()) %>%
    mutate(hour_totals = factor(hour_totals))

# Plot
hour_plot <- ggplot(inc_hour_totals, aes(x = hour_totals, y = n, fill = hour_totals)) +
    geom_col(show.legend = FALSE) +
    labs(title = 'Shooting Incidents per Hour of the Day',
        x = 'Hour (24H)', y = 'Number of Incidents',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')

# Plot side-by-side
grid.arrange(month_plot, hour_plot, nrow = 1)
```

- Like with other crimes, we see a higher amount during the warmer months.
- Shootings are much more likely during nighttime hours.
- Shootings hit a maximum peak at 11:00 (23:00).
- Incidents happen much less often in the morning when people are starting their day and commuting with a minimum at 7:00-9:59.

Looking at these makes me ask the question, "have shootings becoming more or less frequent over the years". 
Let's find out what this data suggests the answer is next.

```{r, Incidents per Year, fig.height = 4, fig.width = 6, fig.align = 'center'}
inc_year_totals <- import_url %>%
    group_by(year_totals = year(DATE)) %>%
    summarize(n = n()) %>%
    mutate(year_totals = factor(year_totals))

year_plot <- ggplot(inc_year_totals, aes(x = year_totals, y = n, fill = year_totals)) +
    geom_col(show.legend = FALSE) +
    theme(axis.text.x = element_text(angle = 25, hjust = 1)) +
    labs(title = 'Shooting Incidents per Year',
        x = 'Year', y = 'Number of Incidents',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
year_plot
```

- The number of shootings consistently trend down until 2020.
- This further shows that there were more shootings during 2006-2014 than 2014-2021.
- Interestingly, the Covid pandemic seems to have affected the number of shooting incidents in the opposite way as would be expected.

Let's take a closer look at 2020 and 2021 but include the months to see if the pandemic had any noticeable influence.
We'll also include 2018-2021 for context.
```{r, Covid Years, fig.height = 4, fig.width = 6, fig.align = 'center'}
options(dplyr.summarise.inform = FALSE)
covid_slice <- import_url %>%
    filter(DATE > '2018-01-01') %>%
    mutate(MONTH = stamp('January', orders = '%B', quiet = TRUE)(DATE)) %>%
    mutate(YEAR = stamp('2020', orders = 'y', quiet = TRUE)(DATE))

covid_months_totals <- covid_slice %>%
    group_by(MONTH, YEAR) %>%
    summarize(n = sum(n())) %>%
    arrange(match(MONTH, month.name)) %>%
    mutate(MONTH = factor(MONTH, levels = month.name))

ggplot(covid_months_totals, aes(x = MONTH, y = n, fill = YEAR)) +
    geom_col() +
    scale_fill_brewer(palette = 'Spectral', direction = 1, name = 'Year') +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = 'Stacked Barchart Shooting Incidents per Year 2018-2021',
        x = 'Month', y = 'Number of Incidents',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
```

Interesting, here it shows there was an increase in shooting incidents beginning in May and June of 2020, coming off a downward trend from previous years.
Let's take another view here and include the date when New York City initiated their lockdown to see if it aligns with this increase.

```{r, Covid Years 2, fig.height = 4, fig.width = 14, fig.align = 'center'}
covid_slice_2 <- import_url %>%
    filter(DATE > '2018-01-01') %>%
    mutate(DATE = stamp('2020-01', orders = 'ym', quiet = TRUE)(DATE))

covid_months_totals_2 <- covid_slice_2 %>%
    group_by(DATE) %>%
    summarize(n = sum(n()))

ggplot(covid_months_totals_2, aes(x = DATE, y = n, fill = DATE)) +
    geom_col(show.legend = FALSE) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
    geom_vline(xintercept = 27.25) +
    annotate('label', x = 27.25, y = 175, angle = 90, color = 'black', label = 'Covid Pandemic Lockdown Initiated') +
    annotate("rect", xmin = 0, xmax = 27.25, ymin = 0, ymax = 250, alpha = .2, fill = "#00c9d0") +
    annotate("rect", xmin = 27.25, xmax = 48.5, ymin = 0, ymax = 250, alpha = .2, fill = "#c42a07") +
    labs(title = 'Shooting Incidents per Year 2018-2021',
        x = 'Month', y = 'Number of Incidents',
        caption = 'Source:<https://catalog.data.gov/dataset/nypd-shooting-incident-data-historic>')
```

There certainly is some underlying correlation here, but from this data we can't say for sure what that is exactly. That said, this is
an interesting result because the increase in shootings seems to correlate to the period of time where the pandemic lockdown in NYC
would have been in affect for over a month and the increase in shootings occurs when restrictions would have been begun to loosen.

### Incident Coordinate Data Visualized on a Map

For reference here is a map of NYC's boroughs in the public domain from Wikipedia: <https://commons.wikimedia.org/w/index.php?title=Special:Redirect/file/5_Boroughs_Labels_New_York_City_Map.svg>
```{r, Reference Map, out.width = '400px', fig.align = 'center'}
reference_map <- 'https://commons.wikimedia.org/w/index.php?title=Special:Redirect/file/5_Boroughs_Labels_New_York_City_Map.svg'
knitr::include_graphics(reference_map)
```

Labels on borough reference map:

1. Manhattan
2. Brooklyn
3. Queens
4. The Bronx
5. Staten Island

Here we're going to visualize the location of each shooting incident using the coordinates given in the dataset. 
First, we can use the minimum and maximum values of the longitudes and latitudes to find the map's bounding box (edges).
Then, use `ggmap()` to generate a map centered around these coordinates. Then, we can use `geom_point()` and `stat_density2d_filled()` to
superimpose our data on the map using the same coordinate system we generated.



## Model

We will now build a model that will try to predict whether a murder will occur depending on the total number of shootings that occur on that day, as well as taking into account the day of the week. This will also require us to add a few more features.
```{r Modeling}

# Add DAY and YEAR features for easier parsing
import_url$DAY <- factor(wday(import_url$DATE, label = TRUE, abbr = FALSE), ordered = FALSE)
import_url$YEAR <- factor(year(import_url$DATE), ordered = FALSE)
# Add TOTAL_TODAY which = the total number of incidents on a given DATE.
import_url <- import_url %>%
    group_by(DATE) %>%
    mutate(TOTAL_TODAY = sum(duplicated(DATE)) + 1)
# Model
murder_model <- glm(MURDER_FLAG ~ TOTAL_TODAY - 1 + DAY, data = import_url, family = 'binomial'); murder_model
plot(murder_model)
model_summary <- summary(murder_model); model_summary
```
- We added 3 new features including `DAY`, `YEAR`, and `TOTAL_TODAY` which equals the total number of incidents on a given `DATE`.
- Using `MURDER_FLAG` and the fact that it is binary (TRUE or FALSE) we used `glm()` and modeled a `binomial` predictor.

Looking at the coefficient p-values etc, maybe a little unsurprisingly, there is a strong relation between day of the week, total shooting incidents per day,
and whether or not a murder occurs, but it is worth confirming.

## Conclusion 

Even features that initially seemed sparse, considering the number of missing entries, led to valuable insights into this data.

Some takeaways we've seen from above:

1. While the overall trend in number of shooting incidents was going down year after year from 2006 to the beginning of 2020, 
they saw a large increase from right after the Covid pandemic began. 
2. Majority of shootings in the dataset occur in Brooklyn, The Bronx, and near public and private housing.
3. Even in low incident areas there are identifiable hotspots that most shootings occur (e.g. Staten Island, though low on the totals scale, shows ~5 localized clusters of incidents).
4. While women make up ~8.18% of the victims in the cases where the sex was known, they only make up ~2.03% of the perpetrators where the sex was known.
5. The north side of Manhattan is responsible for a large portion of the number of incidents in the borough.

### Bias

Much of the bias of this analysis comes from choosing what features to include or not and possibly what conclusions can be drawn.
My experiences while living in two different metropolitan cities for half my life or my past experiences with police officers could also affect the lens I view this information.
Also, ever the optimist I assumed shootings would have seen a reduction during the pandemic, but seeing the data now I can see another story.
In an attempt to mitigate these biases I've chosen to only include what I believe the data to empirically show, and not obfuscate any methods in obtaining these results.


Another bias that is likely present here is how the data was collected, reviewed, and updated. We don't know the exact method in this case, and it is likely
working off of hundreds of police officers' efforts over the years who went about their work with their own biases. These are all things to keep in mind when
working with these datasets and drawing conclusions from them. Hopefully I've made the conclusions made here reproducible and clear.

```{r Session Info}
sessionInfo()
```
